---
title: "Reply Tools AB Test Report"
date: "`r format(Sys.Date(), '%d %B %Y')`"
author: 
- affiliation: "Data Scientist, Wikimedia Foundation"
  name: "Megan Neisler"
output:
  html_document:
    fig_caption: yes
    fig_width: 12
    fig_height: 10
    toc: yes
    toc_depth: 1
    toc_float:
      collapsed: no
  pdf_document:
    citation_package: natbib
    fig_height: 6
    fig_width: 15
    keep_tex: yes
    latex_engine: xelatex
    template: svm-latex-ms.tex
---


# Introduction

The Wikimedia Foundation's [Editing team](https://www.mediawiki.org/wiki/Editing_team#:~:text=The%20Editing%20team%20is%20the,tools%20like%20TemplateData%20and%20Citoid.) is working to improve how contributors communicate on Wikipedia using talk pages through a series of incremental improvements that will be released over time.

As part of this effort, the Editing team introduced a new workflow for replying to specific comments with the intention of making participating productively on talk pages easier and more intuitive. The reply tool is an extra button that appears at the end of a post on a talk page (as shown in the screenshot below). When you click on it, it opens a reply form that automatically signs and indents wikitext talk page comments and offers a quick way for pinging other users among other features.

```{r reply_screenshot, include = FALSE}
example_png <- png::readPNG("Figures/DiscussionTools_Reply_visual_2021.png", info = TRUE)
```

The team ran an AB test of the Reply Tool from 11 February 2021 through 10 March 2021 to assess the efficacy of this new feature, specifically on Junior Contributors (defined as having under 100 cumulative edits). The test included logged-in users that had not used the Reply Tool before (defined as users whose discussiontools-editmode preference is empty) and  attempted an edit at one of the 22 participating Wikipedias (see full list of [participating Wikipedias](https://www.mediawiki.org/wiki/Talk_pages_project#Active_initiatives)). During this test, 50% of all logged-in users at the Wikipedias included in the test had the Reply tool automatically enabled, and 50% did not. Users at these Wikipedias were still able to turn the tool on or off in Special:Preferences. 

Upon conclusion of the test on 10 March 2021, a total of 10,175 edit attempts were initiated by 4,683 distinct users across both test groups.

You can find more information about features of this tool and project updates on the [project page](https://www.mediawiki.org/wiki/Talk_pages_project/Replying).

# Purpose

The primary goal of the AB test was to test the hypothesis that using the reply tool will increase the liklihood of a Junior Contributor publishing their edit without a significant increase in disruption. We will assess disruption by looking at the number of edits made to talk pages that are reverted within 48 hours and the number of editors who are blocked after making an edit to a talk page.

The results of this analysis will be used to determine if the Reply Tool should be deployed to default to all Wiki projects, as an opt-out user preference. Please see further details about the hypotheses and descision scenarios in the [task description](https://phabricator.wikimedia.org/T252057).



# Methodology

```{r setup, include = FALSE}
library(knitr)
library(kableExtra)
library(zeallot) # multi-assignment operator %<-%
library(tidyverse)
# Presentable tables:
library(gt)
library(gtsummary)

# Modelling:
library(lme4)
library(brms)
library(tidybayes)
set.seed(5)

```
The AB test was run on a per wiki basis and users included in the test were randomly assigned to either the control (reply tool disabled by default) or treatment (reply tool enabled by default) based on their user ID. Users within each group also had the option to explicilty turn the tool on or off in their preferences; however, these users remained in the same group they were bucketed in for the duration of the test. 

Data was collected in [EditAttemptStep](https://gerrit.wikimedia.org/r/plugins/gitiles/schemas/event/secondary/+/refs/heads/master/jsonschema/analytics/legacy/editattemptstep/). We excluded data from from Feb 25 to March 1 in this analysis due to an [error](https://gerrit.wikimedia.org/r/c/mediawiki/extensions/WikimediaEvents/+/667690) in the sampling configuration that resulted in the loss of non-reply tool edit events.

In this test, a user can complete a reply using the Reply Tool or using the standard edit workflows (non-reply tool edit). For the purpose of this analysis, these two types of edits are defined as follows: 


**Reply Tool Edit:** Any edit made with the reply tool to a page in a talk namespace. With available instrumentation, we can confirm that all edits made with the reply tool were responses to a comment on the page and not a correction edit. These events were sampled at 100%. 

Recorded in EditAttemptStep as: `event.action = 'init'`, `event.integration = 'discussiontools'`, `event.init_type = 'page'`

See the following Phabricator tickets for further details regarding instrumentation and implementation of the AB test:

* Enable AB Test [T273554](https://phabricator.wikimedia.org/T273554)
* AB Test turned off. [T276967](https://phabricator.wikimedia.org/T276967)
* AB Test Instrumentation Changes [T273096](https://phabricator.wikimedia.org/T273096)
* AB Test QA [T268193](https://phabricator.wikimedia.org/T268193)
* AB Test Bucket Implementation [T268191](https://phabricator.wikimedia.org/T268191)

**Non-Reply Tool Edit:**: Any edit to a page in a talk namespace that were not edits to create new sections or new pages and not made with the reply tool. It's possible that some of the edits were corrective edits (i.e fixing a signature) but current instrumentation does not decipher between this type of edit and a reply. These events were sampled at a rate of 1/16, or 6.125%

Recorded in the EditAttemptStep as:  `event.action = 'init'`, `event.integration = 'page'` , `event.init_type = 'section' or 'page'`, `event.init_mechanism = 'click'`


# Edit Completion Rate by Junior Contributors 

We first calculated the edit completion rate for Junior Contributors by editor type (i.e. Did the user save the edit with or without using the reply tool?) overall and across each participating Wikipedia. For the purposes of this analysis, an edit is considered complete if a user successfully publishes the edit (`event.action = 'saveSuccess'`) after making an edit attempt (`event.action = 'init'`). 


```{r edit_attempt_data, cache=TRUE, include=FALSE}
reply_edit_attempts <-
  read.csv(
    file = 'Data/reply_edit_attempts.csv',
    header = TRUE,
    sep = ",",
    stringsAsFactors = FALSE
  ) # loads all AB test events

# cleanup data
reply_edit_attempts$attempt_dt <-
  as.Date(reply_edit_attempts$attempt_dt, format = "%Y-%m-%d")

#clarfiy levels and lables for factor variables
reply_edit_attempts$editor_type <-
  factor(
    reply_edit_attempts$editor_type,
    levels = c("page", "discussiontools"),
    labels = c("Non-Reply Tool", "Reply Tool")
  )
reply_edit_attempts$edit_count <-
  factor(reply_edit_attempts$edit_count,
         levels = c("under 100", "100-499", "over 500"))

# remove data from dates where EditAttempt Stemps events were not sampled correctly.
reply_edit_attempts  <- reply_edit_attempts %>%
  filter(attempt_dt < '2021-02-25' | attempt_dt > '2021-03-01')

reply_edit_attempts$user_id <-
  as.character(reply_edit_attempts$user_id)
#clarfiy wiki names
reply_edit_attempts <- reply_edit_attempts  %>%
  mutate(
    wiki = case_when(
      #clarfiy participating project names
      wiki == 'frwiki' ~ "French Wikipedia",
      wiki == 'eswiki' ~ "Spanish Wikipedia",
      wiki == 'itwiki' ~ "Italian Wikipedia",
      wiki == 'jawiki' ~ 'Japanese Wikipedia',
      wiki == 'fawiki' ~ 'Persian Wikipedia',
      wiki == 'plwiki' ~ 'Polish Wikipedia',
      wiki == 'hewiki' ~ 'Hebrew Wikipedia',
      wiki == 'nlwiki' ~ 'Dutch Wikipedia',
      wiki == 'hiwiki' ~ 'Hindi Wikipedia',
      wiki == 'kowiki' ~ 'Korean Wikipedia',
      wiki == 'viwiki' ~ 'Vietnamese Wikipedia',
      wiki == 'thwiki' ~ 'Thai Wikipedia',
      wiki == 'ptwiki' ~ 'Portuguese Wikipedia',
      wiki == 'bnwiki' ~ 'Bengali Wikipedia',
      wiki == 'arzwiki' ~ 'Egyptian Wikipedia',
      wiki == 'swwiki' ~ 'Swahili Wikipedia',
      wiki == 'zhwiki' ~ 'Chinese Wikipedia',
      wiki == 'ukwiki' ~ 'Ukrainian Wikipedia',
      wiki == 'idwiki' ~ 'Indonesian Wikipedia',
      wiki == 'amwiki' ~ 'Amharic Wikipedia',
      wiki == 'omwiki' ~ 'Oromo Wikipedia',
      wiki == 'afwiki' ~ 'Afrikaans Wikipedia',
    )
  ) 
```


```{r reply_edit_attempts_jc, echo = FALSE }
# filter date to only look at edit completions by editors with under 100 edits
reply_edit_attempts_jc <- reply_edit_attempts %>%
    filter(edit_count == 'under 100') 
```

## Overall Junior Contributor Edit Completion Rate

```{r edit_completions_all_jc, echo = FALSE}
# Review edit attempts by event type and wiki
edit_completions_all_jc <- reply_edit_attempts_jc %>%
    group_by (editor_type) %>%
    summarise(n_users = n_distinct(user_id),
              n_attempts = n_distinct(edit_attempt_id),
             completion_rate = round(sum(edit_success) / n_attempts, 2),
            completion_rate_pct = paste0(round(sum(edit_success) / n_attempts *100, 1), "%")
  )  %>%
  gt() %>%
  cols_hide(columns = vars(completion_rate)) 

edit_completions_all_jc
```


## Participating Wiki Edit Completion Rate by Junior Contributors 

```{r edit_completions_bywiki_jc, echo = FALSE}
# Review edit attempts by event type and wiki
edit_completions_bywiki_jc <- reply_edit_attempts_jc %>%
  filter(edit_count == 'under 100') %>%
  group_by (wiki, editor_type) %>%
  summarise(n_users = n_distinct(user_id),
            n_attempts = n_distinct(edit_attempt_id),
            completion_rate = round(sum(edit_success) / n_attempts, 2),
            completion_rate_pct = paste0(round(sum(edit_success) / n_attempts *100, 1), "%"),
            groups = 'drop'
  )  
  
```


```{r edit_completions_bywiki_jc_tbl, echo = FALSE}
edit_completions_bywiki_jc_tbl <- edit_completions_bywiki_jc  %>%
  gt() %>%
  cols_hide(columns = vars(completion_rate)) 

edit_completions_bywiki_jc_tbl
```

FIX ME: Work on Table Formatting

```{r jc_reply_edit_completions_bywiki_plot, echo = FALSE}
# Plot edit completion rates for each user on each wiki  

p <- edit_completions_bywiki_jc %>%
    ggplot(aes(x= editor_type, y = completion_rate, fill = editor_type)) +
    geom_col(position = 'dodge') +
    facet_wrap(~ wiki) +
    scale_y_continuous(labels = scales::percent) +
    labs (y = "Percent of edits that were saved ",
          x = "Editor Type",
          title = "Junior Contributor edit completion rate by event type and wiki",
          subtitle = 'Edit completion rate defined as percent of edit attempts that are saved')  +
    theme_bw() +
    scale_fill_brewer(name="Event Type", palette="Set1")  +
    theme(
        plot.title = element_text(hjust = 0.5),
        text = element_text(size=16),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        legend.position = "bottom")
      
p
ggsave("Figures/jc_reply_edit_completions_bywiki_plot.png", p, width = 16, height = 8, units = "in", dpi = 300)
```

Overall Junior Contributors had a much higher edit completion rate using the reply tool. 63.3% of all edit attempts made with the reply tool were successfully saved, compared to 27.8% of all non-reply tool edit attempts.

This trend is reflected across each participating wiki as well. Junior Contributors have a higher edit completion rate than non-reply tool edits on each participating wiki. 

Note: There were very few AB test events recorded for Swahuli, Afrikaans, and Egyptian Wikipedia and no recorded AB test events for Amharic and Oromo Wikipedia. As a result, we are not able to conclude any effects from the reply tool on these specific populations. FIXME: ADD AS FOOTNOTE TO TABLE.

FIXME: Add additional insights regarding trends.


## Simple Logistic Regression Model

We next explored different models to correctly infer the impact of the reply tool on whether an edit was completed or not and account for random effects by the user and wiki.

We first started with just a standard linear regression where use of the reply tool is the only predictor. In this case, we used a logistic regression model as the outcome (edit complete or not complete) is a binary variable. This model treats all edit attempts as independent and identically distributed and does not account from the effect of the user on the success probability of an edit.

```{r glm_fit_jc, echo = FALSE}
glm_fit_jc <- glm(
    formula = edit_success ~ reply_tool_used, 
    family = binomial(link = 'logit'),
    data = reply_edit_attempts_jc, 
             )
```

```{r glm_summary_table_jc, results='asis', echo=FALSE}
glm_fit_jc_tbl <- glm_fit_jc %>%
  tbl_regression(
    label = list(new_interface = "Using reply tool"),
    intercept = TRUE
  )
glm_fit_jc_tbl
```


Since the model parameters are on the log-odds scale, we need to take the exponentiation of the effect (exp(Î²~1~)) to determine the multiplicative effect on the odds of an edit getting published.

The estimates provided by this model indicate that the odds of publishing the edit are multiplied by 2.27 when using the new reply tool; however, this does not take into account the effects of the user and the wiki on edit completion rates.


## Bayesian Hierarchical Logistic Regression Model

Edit attempts completed on the same wiki and by the users on that wiki are related to each other. Therefore, we can more accurately infer the impact of the reply tool by incorporating the effects of the user and wiki into the model.

We used a [Bayesian Hierarchical regression model](https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling) to model this structure since individual edit attempts in our observed dataset are grouped. In this model, the user and wiki are random effects and whether the reply tool was used is the fixed effect or predictor variable. A Bayesian approach was used so that we can reason about the parameters (and other quantities) probabilistically.

```{r priors, echo = FALSE}
priors <- c(
  set_prior(prior = "std_normal()", class = "b"),
  set_prior("cauchy(0, 5)", class = "sd")
)


```


```{r  hrm_fit_jc, echo = FALSE, include = FALSE}
hrm_fit_jc <- brm(
  edit_success ~ reply_tool_used + (1 | wiki/user_id),
  family = bernoulli(link = "logit"),
  data = reply_edit_attempts_jc,
  prior = priors,
  chains = 4, cores = 4
)
```

```{r}
summary(hrm_fit_jc )
```


```{r hrm_summary_table_jc, results='asis', echo=FALSE}
hrm_fit_jc_tbl <- hrm_fit_jc  %>%
  spread_draws(b_reply_tool_used, b_Intercept) %>%
  mutate(
    exp_b = exp(b_reply_tool_used),
    b4 = b_reply_tool_used / 4,
    avg_lift =  plogis(b_Intercept + b_reply_tool_used) - plogis(b_Intercept)
  ) %>%
  pivot_longer(
    b_reply_tool_used:avg_lift,
    names_to = "param",
    values_to = "val"
  ) %>%
  group_by(param) %>%
  summarize(
    ps = c(0.025, 0.5, 0.975),
    qs = quantile(val, probs = ps),
    .groups = "drop"
  ) %>%
  mutate(
    quantity = ifelse(
      param %in% c("b_Intercept", "b_reply_tool_used"),
      "Parameter", "Function of parameter(s)"
    ),
    param = factor(
      param,
      c("b_Intercept", "b_reply_tool_used", "exp_b", "b4", "avg_lift"),
      c("(Intercept)", "Using reply tool", "Multiplicative effect on odds", "Divide-by-4 rule", "Average lift")
    ),
    ps = factor(ps, c(0.025, 0.5, 0.975), c("lower", "median", "upper")),
  ) %>%
  pivot_wider(names_from = "ps", values_from = "qs") %>%
  arrange(quantity, param)

hrm_fit_jc_tbl%>%
  gt(rowname_col = "param", groupname_col = "quantity") %>%
  row_group_order(c("Parameter", "Function of parameter(s)")) %>%
  fmt_number(vars(lower, median, upper), decimals = 3) %>%
  fmt_percent(columns = vars(median, lower, upper), rows = 2:3, decimals = 1) %>%
  cols_align("center", vars(median, lower, upper)) %>%
  cols_merge(vars(lower, upper), pattern = "({1}, {2})") %>%
  cols_move_to_end(vars(lower)) %>%
  cols_label(median = "Point Estimate", lower = "95% CI") %>%
  tab_style(cell_text(weight = "bold"), cells_row_groups()) %>%
  tab_footnote("CI: Credible Interval", cells_column_labels(vars(lower))) %>%
  tab_footnote(
    html("Average lift = Pr(Success|New interface) - Pr(Success|Old interface) = logit<sup>-1</sup>(&beta;<sub>0</sub> + &beta;<sub>1</sub>) - logit<sup>-1</sup>(&beta;<sub>0</sub>)"),
    cells_body(vars(median), 3)
  ) %>%
  tab_header("Posterior summary of model parameters")
```


Since the model parameters are on the log-odds scale, we needed to apply transformations to make sense of them. We used the "divide-by4" rule suggested by Gelman, Hill, and Vehtari 2021. [FIXME: LINK TO FULL REFERENCE) to approximate the maximum increase in the probability of success corresponding to which editor type (reply tool or non-reply tool) was used. Using the bayesian model, we can also directly calculate the average lift.

The model approximates the maximum lift in edit attempt success probability is 20.3% (95% CI: 17.2%-23.4%) using the divide-by-4-rule and calculates an average lift of 16.5% (95% CI: 14.2%-18.8%); and we can confirm statistical significance at the 0.05 level (as indicated by a credible interval that does not cross 1). In other words, there is an average 16.5% increase in the probability of an edit being completed when a Junior Contributor uses the Reply Tool.

The model also indicates that the odds of saving the edit are multiplied by 2.25 (95% CI: 1.2-2.55) when using the reply tool. We can also confirm statistical significance (at the 0.05 level).

# Edit Completion Rate Across all Contributor Experience Levels

As the purpose of this test, we first focused on determining if the reply tool had an impact on Junior Contributors' edit completion rate; however, we also reviewed the reply tool impact across all contributors' edit completion rates to provide insight into differences due to experience level.

```{r reply_edit_completions_all, echo = FALSE}
reply_edit_completions_all <- reply_edit_attempts %>%
  group_by (editor_type) %>%
  summarise(
    n_users = n_distinct(user_id),
    n_attempts = n_distinct(edit_attempt_id),
    completion_rate = round(sum(edit_success) / n_attempts, 2),
    completion_rate_pct = paste0(round(sum(edit_success) / n_attempts *100, 1), "%")
  )  %>%
  gt() %>%
  tab_header(
    "Overall edit completion rate",
      "Across all Editor Experience Levels"
  ) %>%
  cols_hide(columns = vars(completion_rate)) 

reply_edit_completions_all
#FIXME: Work on Table Formatting
```



## Participating Wiki edit completion rate
```{r reply_edit_completions_bywiki, echo = FALSE}
# Review edit attempts by event type and wiki
reply_edit_completions_bywiki <- reply_edit_attempts %>%
  group_by (wiki, editor_type) %>%
  summarise(
    n_users = n_distinct(user_id),
    n_attempts = n_distinct(edit_attempt_id),
    completion_rate = round(sum(edit_success) / n_attempts, 1),
    completion_rate_pct = paste0(round(sum(edit_success) / n_attempts *100, 1), "%"),
    .groups = 'drop'
  )  
```

```{r reply_edit_completions_bywiki_tbl, echo = FALSE}
reply_edit_completions_bywiki_tbl <-
  reply_edit_completions_bywiki %>%
  gt() %>% 
   tab_header(
    "Edit completion rate by participating wiki",
      "Across all Editor Experience Levels"
  ) %>%
  cols_hide(columns = vars(completion_rate)) 

reply_edit_completions_bywiki_tbl
```

FIXME: Work on Table Formatting

```{r reply_edit_completions_bywiki_plot, echo = FALSE}
# Plot edit completion rates for each user on each wiki and each 

p <- reply_edit_completions_bywiki  %>%
  ggplot(aes(x = editor_type, y = completion_rate, fill = editor_type)) +
  geom_col(position = 'dodge') +
  facet_wrap( ~ wiki) +
  scale_y_continuous(labels = scales::percent) +
  labs (
    y = "Percent of edits that were saved ",
    x = "Editor Type",
    title = "Edit completion rate by event type and wiki",
    subtitle = 'Edit completion rate defined as percent of edit attempts that are saved'
  )  +
  theme_bw() +
  scale_fill_brewer(name = "Event Type", palette = "Set1")  +
  theme(
    plot.title = element_text(hjust = 0.5),
    text = element_text(size = 16),
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    legend.position = "bottom"
  )

p
ggsave(
  "Figures/reply_edit_completions_bywiki_plot.png",p, width = 16, height = 8, units = "in", dpi = 300
)
```

The reply tool has a similar completion rate for contributors across all experience levels (61.2%) as it does for Junior Contributors (63.3%); however, the completion rate for the non-reply tool edits is much higher. Only 27% of edit attempts by Junior Contributors were completed using non-reply tool edits compared to 57.8% by all contributors.

The results vary on a per wiki basis. The reply tool had a higher edit completion rate on all participating wikis except Ukranian Wikipedia, Hebrew Wikipedia, Italian Wikipedia, and Spanish Wikipedia.

Note: There were very few AB test events recorded for Swahili, Afrikaans, and Egyptian Wikipedia and no recorded AB test events for Amharic and Oromo Wikipedia. As a result, we are not able to conclude any effects from the reply tool on these specific Wikis. 

FIXME: Add additional insights regarding trends.

### Edit Completion Rate By Experience Level

```{r reply_edit_completions_byexp, echo = FALSE}
# Review edit attempts by event type and wiki
reply_edit_completions_byexperience <- reply_edit_attempts %>%
  group_by (edit_count, editor_type) %>%
  summarise(
    n_users = n_distinct(user_id),
    n_attempts = n_distinct(edit_attempt_id),
    completion_rate = round(sum(edit_success) / n_attempts, 1),
    completion_rate_pct = paste0(round(sum(edit_success) / n_attempts *100, 1), "%"),
    .groups = 'drop'
  )  
```

```{r reply_edit_completions_byexp_tbl, echo = FALSE}
reply_edit_completions_byexperience_tbl <-
  reply_edit_completions_byexperience %>%
  gt() %>% 
   tab_header(
    "Edit completion rate by experience level and editor type",
      "Across all participating wikis"
  ) %>%
  cols_hide(columns = vars(completion_rate)) 

reply_edit_completions_byexperience_tbl

#FIXME: Work on table formatting
```


```{r reply_edit_completions_byexperiencelevel_plot, echo = FALSE}
# plot proportion of edit complets by use of reply tool and experience level

p <- reply_edit_completions_byexperience  %>%
  ggplot(aes(x= editor_type , y = completion_rate, fill = editor_type)) +
  geom_col(position = 'dodge') +
  scale_y_continuous(labels = scales::percent) +
  facet_grid(~ edit_count) +
  labs (y = "Percent of edit attempts saved",
        x = "Editor Type",
        title = "Percent of edit attempts completed by user experience and editor tool type")  +
  theme_bw() +
  scale_fill_brewer(name="Editor Type", palette="Set1")  +
   theme(
    plot.title = element_text(hjust = 0.5),
    text = element_text(size = 16),
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    legend.position = "bottom"
  )

p

ggsave("Figures/reply_edit_completions_byexperiencelevel.png", p, width = 16, height = 8, units = "in", dpi = 300)
```


The figure above [FIXME: ADD FIGURE CAPTION AND #] compares the proportion of edit attempts that were completed by editor type (reply tool or non-reply tool) and the user's cumulative edit count. The data suggests that more user experience is associated with a higher likelihood of edit completion rate. 

Junior Contributors (users with under 100 cumulative edits) saw the biggest increase  (130% increase) in edit completion rate with the Reply tool compared to non-reply tool edits. In contrast, Senior Contributors (users with over 500 cumulative edits) had a slightly higher edit completion rate (29.2% increase) when they did not use the reply tool. This likely reflects Senior Contributors increased comfort level and experience using the old interface. 

FIXME: Expand synopsis

## Simple Logistic Regression Model 

To infer the impact of the reply tool, we applied the same models used when assessing the impact on Junior Contributor edit completion rate except we included edits attempts by all contributors in the model. 

```{r glm_fit, echo = FALSE}
glm_fit <- glm(
  formula = edit_success ~ reply_tool_used, 
  family = binomial(link = 'logit'),
  data = reply_edit_attempts, 
             )
```

```{r glm_summary_table, results='asis', echo=FALSE}
glm_fit_tbl <- glm_fit %>%
  tbl_regression(
    label = list(new_interface = "Using reply tool"),
    intercept = TRUE
  )
glm_fit_tbl
```


The estimates provided by the standard linear regression model indicate that odds of a user completing an edit with the reply tool is slightly higher (1.02 times) than the odds of a user completing an edit using page or section editing; however, the confidence interval crosses 1 indicating the difference between the two groups is not statistically different. It also does not take into account the effects of the user and the wiki on edit completion rates.

## Bayesian Hierarchical Logistic Regression Model


```{r  hrm_fit, echo = FALSE, include = FALSE}

hrm_fit <- brm(
  edit_success ~ reply_tool_used + (1 | wiki/user_id), # To account for both wiki and user_id, we allow the intercept, represented by 1, to vary among  wiki and users within the wiki.
  family = bernoulli(link = "logit"),
  data = reply_edit_attempts,
  prior = priors,
  chains = 4, cores = 4
)
```

```{r}
summary(hrm_fit)
```


```{r hrm_summary_table, results='asis', echo=FALSE}
hrm_fit_tbl <- hrm_fit  %>%
  spread_draws(b_reply_tool_used, b_Intercept) %>%
  mutate(
    exp_b = exp(b_reply_tool_used),
    b4 = b_reply_tool_used / 4,
    avg_lift =  plogis(b_Intercept + b_reply_tool_used) - plogis(b_Intercept)
  ) %>%
  pivot_longer(
    b_reply_tool_used:avg_lift,
    names_to = "param",
    values_to = "val"
  ) %>%
  group_by(param) %>%
  summarize(
    ps = c(0.025, 0.5, 0.975),
    qs = quantile(val, probs = ps),
    .groups = "drop"
  ) %>%
  mutate(
    quantity = ifelse(
      param %in% c("b_Intercept", "b_reply_tool_used"),
      "Parameter", "Function of parameter(s)"
    ),
    param = factor(
      param,
      c("b_Intercept", "b_reply_tool_used", "exp_b", "b4", "avg_lift"),
      c("(Intercept)", "Using reply tool", "Multiplicative effect on odds", "Divide-by-4 rule", "Average lift")
    ),
    ps = factor(ps, c(0.025, 0.5, 0.975), c("lower", "median", "upper")),
  ) %>%
  pivot_wider(names_from = "ps", values_from = "qs") %>%
  arrange(quantity, param)

hrm_fit_tbl%>%
  gt(rowname_col = "param", groupname_col = "quantity") %>%
  row_group_order(c("Parameter", "Function of parameter(s)")) %>%
  fmt_number(vars(lower, median, upper), decimals = 3) %>%
  fmt_percent(columns = vars(median, lower, upper), rows = 2:3, decimals = 1) %>%
  cols_align("center", vars(median, lower, upper)) %>%
  cols_merge(vars(lower, upper), pattern = "({1}, {2})") %>%
  cols_move_to_end(vars(lower)) %>%
  cols_label(median = "Point Estimate", lower = "95% CI") %>%
  tab_style(cell_text(weight = "bold"), cells_row_groups()) %>%
  tab_footnote("CI: Credible Interval", cells_column_labels(vars(lower))) %>%
  tab_footnote(
    html("Average lift = Pr(Success|New interface) - Pr(Success|Old interface) = logit<sup>-1</sup>(&beta;<sub>0</sub> + &beta;<sub>1</sub>) - logit<sup>-1</sup>(&beta;<sub>0</sub>)"),
    cells_body(vars(median), 3)
  ) %>%
  tab_header("Posterior summary of model parameters")
```

The hierarchical regression model has estimated a slightly larger effect of the reply tool on edit completion rate after controlling for the random effects by the wiki and users within each wiki.  We can also now confirm the statistically significant association between whether the reply tool is used and the odds of completing an edit.

For edit attempts by all contributors, the model approximates the maximum lift in edit attempt success probability due to the reply tool is 1.3% (95% CI: -0.4%-2.9%) and calculates an average lift of 1.2% (95% CI: -0.4%-2.7%); however, we can not confirm statistical significance at the 0.05 level (as indicated by a credible intervals that cross 1).

The odds of saving the edit are multiplied by 1.05 (95% CI: 0.98-1.12) when using the reply tool. We can confirm statistical signficance for this estimate (at the 0.05 level) since the credible interval does not contain 1 (a multiplicative effect of 1 is no change either way).

